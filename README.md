# üìò KCC-A Uma variante do KCC

> **Autor**: Tiago Marcelino Reis
> **Programa de P√≥s-Gradua√ß√£o**: Ci√™ncia da Computa√ß√£o  
> **Institui√ß√£o**: Universidade Federal de Goi√°s
> **Disciplina**: T√≥picos Especiais em Redes  
> **Orientador**: Ant√¥nio Carlos de Oliveira J√∫nior
> **Ano**: 2025  

---

## üìë Sum√°rio

- [1. Introdu√ß√£o](#1-introdu√ß√£o)
- [2. Fundamenta√ß√£o Te√≥rica](#2-fundamenta√ß√£o-te√≥rica)
- [3. Trabalhos Relacionados](#3-trabalhos-relacionados)
- [4. Metodologia](#4-metodologia)
- [5. Desenvolvimento](#5-desenvolvimento)
- [6. Resultados e Discuss√µes](#6-resultados-e-discuss√µes)
- [7. Apresenta√ß√£o dos Gr√°ficos e Execu√ß√£o](#7-Apresenta√ß√£o-dos-Gr√°ficos-e-Execu√ß√£o)
- [8.Considera√ß√µes Finais](#Considera√ß√µes-Finais)
- [9. Refer√™ncias](#9-refer√™ncias)

---

## 1. Introdu√ß√£o

A crescente sofistica√ß√£o dos ataques cibern√©ticos e a escassez de profissionais qualificados em seguran√ßa da informa√ß√£o t√™m impulsionado a pesquisa por solu√ß√µes que automatizem e otimizem as opera√ß√µes de red teaming. Tradicionalmente, o red teaming envolve equipes especializadas que simulam ataques a sistemas e redes, a fim de identificar vulnerabilidades e testar a efic√°cia das defesas existentes. Contudo, a complexidade e o tempo demandados por essas opera√ß√µes manuais limitam sua escalabilidade e a capacidade de adapta√ß√£o a cen√°rios de amea√ßas em constante evolu√ß√£o.

Nesse contexto, a integra√ß√£o de t√©cnicas de intelig√™ncia artificial, particularmente o Aprendizado por Refor√ßo (RL), emerge como uma abordagem promissora para a automa√ß√£o de opera√ß√µes de red teaming. O RL permite que um agente aprenda a tomar decis√µes sequenciais em um ambiente din√¢mico, otimizando uma recompensa espec√≠fica, o que se alinha intrinsecamente √† natureza estrat√©gica e t√°tica dos ataques cibern√©ticos. No entanto, a aplica√ß√£o direta de algoritmos de RL convencionais a cen√°rios de seguran√ßa ofensiva, especialmente em ambientes estoc√°sticos e de mundo real como os de Capture The Flag (CTF), apresenta desafios significativos, como a complexidade da modelagem do ambiente, a escassez de dados para treinamento e a necessidade de identificar sequ√™ncias de ataque √≥timas e furtivas.

Este trabalho aborda tais desafios por meio da implementa√ß√£o e an√°lise do projeto Exoskeleton, uma camada de abstra√ß√£o projetada para automatizar opera√ß√µes de red teaming em cen√°rios de CTF, atuando como uma interface entre algoritmos de aprendizado por refor√ßo e m√°quinas vulner√°veis alvo. O Exoskeleton √© equipado com um arsenal de t√°ticas, t√©cnicas e procedimentos (TTPs) de red teaming, visando facilitar ataques automatizados. No cerne do Exoskeleton est√° o algoritmo Kill Chain Unscrambler (KCU), uma abordagem inovadora que emprega √°rvores de decis√£o em vez de redes neurais para a identifica√ß√£o de sequ√™ncias de ataque. Adicionalmente, este trabalho explora o Kill Chain Catalyst (KCC), um m√≥dulo catal√≠tico inspirado em bioinform√°tica, que visa acelerar a converg√™ncia da curva de aprendizado do KCU, otimizando a busca por sequ√™ncias de ataque com menor n√∫mero de passos e minimizando erros.

A relev√¢ncia deste estudo reside na demonstra√ß√£o pr√°tica da viabilidade de sistemas aut√¥nomos para red teaming, contribuindo para o avan√ßo da seguran√ßa cibern√©tica ao possibilitar a detec√ß√£o proativa de vulnerabilidades e a valida√ß√£o cont√≠nua de defesas. Al√©m da implementa√ß√£o e an√°lise do Exoskeleton e seus componentes, este trabalho prop√µe uma varia√ß√£o do algoritmo KCC, que ser√° objeto de pesquisa de mestrado, buscando aprimorar ainda mais a efici√™ncia e a adaptabilidade das opera√ß√µes aut√¥nomas de red teaming em cen√°rios din√¢micos.

---

## 2. Fundamenta√ß√£o Te√≥rica

Este cap√≠tulo estabelece os conceitos fundamentais que servem de base para a compreens√£o do projeto Exoskeleton e das metodologias empregadas na automa√ß√£o de opera√ß√µes de red teaming. Ser√£o abordados os princ√≠pios do red teaming, o paradigma do aprendizado por refor√ßo (RL) e, em particular, os algoritmos de RL utilizados, bem como os conceitos subjacentes aos algoritmos Kill Chain Unscrambler (KCU) e Kill Chain Catalyst (KCC).

### 2.1 Red Teaming
O red teaming √© uma pr√°tica de seguran√ßa ofensiva que simula ataques reais a sistemas, redes, aplica√ß√µes ou pessoas, com o objetivo de identificar e explorar vulnerabilidades, testar a resili√™ncia das defesas e a capacidade de resposta de uma organiza√ß√£o. Diferentemente de testes de penetra√ß√£o convencionais, o red teaming adota uma abordagem mais abrangente, buscando replicar as t√°ticas, t√©cnicas e procedimentos (TTPs) de advers√°rios reais, muitas vezes operando com informa√ß√µes limitadas sobre o ambiente alvo. A meta principal √© fornecer uma avalia√ß√£o realista da postura de seguran√ßa, revelando pontos cegos e falhas que poderiam ser explorados por atacantes maliciosos.

### 2.2 Aprendizado por Refor√ßo (Reinforcement Learning - RL)
O Aprendizado por Refor√ßo √© uma √°rea do aprendizado de m√°quina onde um agente aprende a tomar decis√µes em um ambiente para maximizar uma medida de recompensa. Em RL, o agente interage com o ambiente, observando seu estado, realizando uma a√ß√£o e recebendo uma recompensa (positiva ou negativa) e um novo estado. O objetivo do agente √© aprender uma pol√≠tica √≥tima, que mapeia estados a a√ß√µes, para acumular a maior quantidade de recompensa ao longo do tempo. Os componentes chave do RL incluem:

Agente: O tomador de decis√µes.

Ambiente: O mundo com o qual o agente interage.

Estado (State): Uma representa√ß√£o do ambiente em um determinado momento.

A√ß√£o (Action): Uma decis√£o tomada pelo agente que afeta o estado do ambiente.

Recompensa (Reward): Um sinal num√©rico que o ambiente fornece ao agente, indicando o qu√£o boa ou ruim foi uma a√ß√£o.

Pol√≠tica (Policy): Uma fun√ß√£o que mapeia estados a a√ß√µes, determinando o comportamento do agente.

Algoritmos de RL tradicionais, como Proximal Policy Optimization (PPO), Advantage Actor-Critic (A2C) e Deep Q-Networks (DQN), tipicamente empregam redes neurais para aproximar a fun√ß√£o de valor ou a pol√≠tica.

### 2.3 Kill Chain Unscrambler (KCU)
O algoritmo Kill Chain Unscrambler (KCU) √© uma abordagem de Aprendizado por Refor√ßo projetada especificamente para endere√ßar as limita√ß√µes de algoritmos de RL convencionais na sequencia√ß√£o de tarefas em ambientes estoc√°sticos, como os cen√°rios de Capture The Flag (CTF). Diferente de PPO, A2C e DQN, o KCU adota uma abordagem n√£o-ortodoxa, utilizando o algoritmo de √Årvore de Decis√£o em vez de redes neurais como seu motor. Essa escolha visa aprimorar o desempenho do algoritmo tanto no treinamento quanto na aplica√ß√£o, especialmente na identifica√ß√£o de sequ√™ncias de ataque em ambientes de mundo real, onde o cen√°rio alvo √© desconhecido. O KCU busca alcan√ßar uma sequ√™ncia √≥tima em poucos passos, minimizando falhas para evitar a detec√ß√£o.

A implementa√ß√£o do algoritmo KCU √© realizada atrav√©s da Decision Tree Policy, que por sua vez implementa o Tree classifier utilizando Random Forest para os processos de fitting e predi√ß√£o. Este m√≥dulo √© respons√°vel por capturar e aprender os melhores padr√µes a partir de dados observados, passando por um processo de transforma√ß√£o KCU para realizar predi√ß√µes sem depender de redes neurais. A classe KCU estende a BaseAlgorithm do Stable Baselines3 e orquestra a funcionalidade geral do algoritmo. Durante o treinamento, ela coleta rollouts, filtra as melhores sequ√™ncias e, ent√£o, treina a pol√≠tica selecionada.

### 2.4 Kill Chain Catalyst (KCC)
O Kill Chain Catalyst (KCC) √© um m√≥dulo catal√≠tico integrado ao processo KCU, inspirado em conceitos de bioinform√°tica, processos de alinhamento gen√©tico e o uso de enzimas para acelerar a decomposi√ß√£o de prote√≠nas. O objetivo primordial do KCC √© acelerar a converg√™ncia da curva de aprendizado dentro do KCU, buscando sequ√™ncias com um n√∫mero reduzido de passos e, crucialmente, minimizando erros no encadeamento de t√©cnicas de ataque para alcan√ßar sequ√™ncias de ataque mais furtivas e realistas.

Nesse processo, o alinhamento gen√©tico, especificamente o algoritmo Needleman-Wunsch, desempenha um papel central no alinhamento das kill chains, que s√£o tratadas como prote√≠nas. O consenso dessas sequ√™ncias alinhadas gera pesos para cada t√©cnica. Este vetor de pesos atribui import√¢ncia a cada item na sequ√™ncia dentro de uma amostra de treinamento, funcionando como um processo enzim√°tico para acelerar a busca por sequ√™ncias √≥timas de kill chain.

A combina√ß√£o do KCU com o KCC representa um avan√ßo significativo na automa√ß√£o de red teaming, permitindo que agentes de RL aprendam a executar ataques complexos de forma mais eficiente e adapt√°vel a ambientes reais.

---

## 3. Trabalhos Relacionados

A automa√ß√£o de opera√ß√µes de red teaming e a aplica√ß√£o de intelig√™ncia artificial em ciberseguran√ßa s√£o campos de pesquisa em ascens√£o, com diversos trabalhos que buscam otimizar e escalar as capacidades defensivas e ofensivas. Este cap√≠tulo discute a literatura existente e como o projeto Exoskeleton se posiciona em rela√ß√£o a esses esfor√ßos.

Tradicionalmente, a pesquisa em seguran√ßa cibern√©tica com foco em automa√ß√£o tem se concentrado em sistemas de detec√ß√£o de intrus√£o, preven√ß√£o de ataques e resposta a incidentes. No entanto, a perspectiva de automatizar o lado ofensivo, ou red teaming, tem ganhado tra√ß√£o recentemente. Muitos dos trabalhos iniciais nessa √°rea exploraram a utiliza√ß√£o de scripts automatizados e frameworks de testes de penetra√ß√£o para simular ataques. Embora eficazes para tarefas espec√≠ficas, essas abordagens carecem da adaptabilidade e da capacidade de aprendizado necess√°rias para cen√°rios din√¢micos e desconhecidos.

Com o advento do Aprendizado de M√°quina, e em particular do Aprendizado por Refor√ßo (RL), novas possibilidades surgiram. Diversos estudos t√™m explorado o uso de RL para tarefas como reconhecimento de rede, explora√ß√£o de vulnerabilidades e movimento lateral. Por exemplo, alguns trabalhos prop√µem agentes de RL que aprendem a navegar em redes e a identificar alvos de alto valor. Outros se concentram em otimizar a sele√ß√£o de exploits ou a sequ√™ncia de a√ß√µes para atingir um objetivo espec√≠fico. Contudo, muitos desses estudos s√£o conduzidos em ambientes simulados ou emulados, onde o espa√ßo de estados e a√ß√µes √© bem definido e conhecido. A transi√ß√£o para ambientes de mundo real, com suas complexidades e estocasticidade, permanece um desafio significativo.

A literatura tamb√©m inclui trabalhos que utilizam algoritmos de RL convencionais, como PPO, A2C e DQN, em cen√°rios de seguran√ßa ofensiva. No entanto, o desempenho desses algoritmos pode ser limitado em ambientes complexos e com sparse rewards, caracter√≠sticas comuns em CTFs e outras simula√ß√µes de red teaming. A necessidade de um grande volume de dados para treinamento e a dificuldade em interpretar as decis√µes de redes neurais s√£o pontos de aten√ß√£o.

O Exoskeleton se diferencia desses trabalhos ao propor uma camada de abstra√ß√£o entre algoritmos de RL e m√°quinas vulner√°veis reais, focando especificamente em cen√°rios de Capture The Flag. A integra√ß√£o de dezenas de TTPs de red teaming diretamente na estrutura do Exoskeleton permite uma maior granularidade e realismo nas opera√ß√µes automatizadas. Al√©m disso, a abordagem inovadora do Kill Chain Unscrambler (KCU), que emprega √°rvores de decis√£o em vez de redes neurais, distingue o Exoskeleton dos trabalhos que dependem exclusivamente de modelos baseados em redes neurais. Essa escolha visa otimizar o desempenho do algoritmo no treinamento e na aplica√ß√£o, superando limita√ß√µes na identifica√ß√£o de sequ√™ncias de ataque em ambientes desconhecidos.

A introdu√ß√£o do Kill Chain Catalyst (KCC), um m√≥dulo inspirado em bioinform√°tica para acelerar a converg√™ncia do KCU, tamb√©m representa um ponto de inova√ß√£o em rela√ß√£o aos trabalhos existentes. A utiliza√ß√£o do alinhamento gen√©tico para ponderar t√©cnicas em uma sequ√™ncia de ataque √© uma abordagem novel que visa tornar as opera√ß√µes de red teaming mais eficientes e furtivas.

Em suma, enquanto a pesquisa na automa√ß√£o de red teaming e na aplica√ß√£o de RL em ciberseguran√ßa continua a evoluir, o Exoskeleton, com seus componentes KCU e KCC, oferece uma perspectiva √∫nica ao abordar as complexidades de ambientes de mundo real e a necessidade de sequ√™ncias de ataque otimizadas e furtivas, posicionando-se como uma contribui√ß√£o relevante e inovadora no campo.

---

## 4. Metodologia

Este cap√≠tulo detalha a metodologia empregada na implementa√ß√£o e avalia√ß√£o do projeto Exoskeleton, incluindo a estrutura do sistema, os algoritmos de Aprendizado por Refor√ßo utilizados, os ambientes de teste e os procedimentos para a execu√ß√£o dos experimentos.

### 4.1 Arquitetura do Exoskeleton
O Exoskeleton atua como uma camada de abstra√ß√£o entre os algoritmos de Aprendizado por Refor√ßo (RL) e as m√°quinas vulner√°veis alvo, especificamente projetado para cen√°rios de Capture The Flag (CTF). Sua arquitetura √© modular, permitindo a integra√ß√£o flex√≠vel de diferentes algoritmos de RL e a expans√£o de t√°ticas, t√©cnicas e procedimentos (TTPs) de red teaming.

O projeto Exoskeleton √© composto pelos seguintes m√≥dulos principais:

Exoskeleton Core (core.exoskeleton): Representa o ambiente de RL, implementando a interface Gymnasium (anteriormente OpenAI Gym) para permitir a intera√ß√£o com algoritmos de RL padronizados. Ele gerencia o estado do ambiente, processa as a√ß√µes do agente e calcula as recompensas. O Exoskeleton √© equipado com dezenas de TTPs de red teaming para facilitar ataques automatizados.

Algoritmos de RL: Inclui implementa√ß√µes dos agentes PPO (agent_ppo.py) e KCU (agent_kcu.py), que interagem com o ambiente Exoskeleton.

Pol√≠ticas: A DecisionTreePolicy (models.policy) √© a implementa√ß√£o central do KCU, utilizando Random Forest como classificador de √°rvore.

M√≥dulos Auxiliares: Contempla callbacks (core.callback) para monitoramento do processo de aprendizado e classes auxiliares para a gest√£o do ambiente.

### 4.2 Ambientes de Teste
Para a avalia√ß√£o do Exoskeleton, foram utilizados ambientes de teste pr√©-configurados em cont√™ineres Docker, oferecendo cen√°rios controlados e replic√°veis. Os cen√°rios inclu√≠dos no projeto s√£o:

Cen√°rio dummy: Um ambiente simplificado para testes iniciais e demonstra√ß√£o da funcionalidade do Exoskeleton. A Kill Chain √≥tima para este cen√°rio inclui: ScanningIPBlocks, WordlistScanning, EmailAddresses, PasswordGuessingSSH, LocalAccountsSSH, FileandDirectoryDiscoverySSH, SetuidandSetgidSSH, DatafromLocalSystemSSH.

Cen√°rio CVE-2018-10993: Um ambiente que simula uma vulnerabilidade real (CVE-2018-10993), permitindo testes em um contexto de seguran√ßa mais realista. A Kill Chain √≥tima para este cen√°rio envolve: ScanningIPBlocks, LibSSHUnauthorizedAccess, DatafromLocalSystemSSH.

Al√©m dos ambientes espec√≠ficos do Exoskeleton, o projeto tamb√©m demonstra a compatibilidade com o ambiente de simula√ß√£o de rede NASim (nasim.make_benchmark("tiny", fully_obs=False)) para avalia√ß√£o dos agentes PPO e KCU em um contexto mais abstrato de rede.

### 4.3 Algoritmos de Aprendizado por Refor√ßo
Foram empregados e comparados os seguintes algoritmos de RL:

PPO (Proximal Policy Optimization): Um algoritmo de RL baseado em pol√≠tica, conhecido por sua estabilidade e bom desempenho em uma variedade de tarefas. Utiliza redes neurais para aprender a pol√≠tica √≥tima.

KCU (Kill Chain Unscrambler): O algoritmo principal do projeto, que se diferencia por utilizar √°rvores de decis√£o (Decision Tree Policy com Random Forest) em vez de redes neurais. O KCU √© projetado para lidar com a estocasticidade e a descoberta de sequ√™ncias de ataque em ambientes desconhecidos.

KCC (Kill Chain Catalyst): Um m√≥dulo catal√≠tico que, integrado ao KCU, visa acelerar a converg√™ncia do aprendizado e otimizar a busca por sequ√™ncias de ataque, utilizando conceitos de alinhamento gen√©tico e pondera√ß√£o de t√©cnicas.

### 4.4 Procedimentos Experimentais
Os experimentos foram conduzidos seguindo os seguintes passos:

Configura√ß√£o do Ambiente: Os ambientes de teste (dummy e CVE-2018-10993) foram inicializados utilizando Docker Compose para garantir consist√™ncia e isolamento.

Instala√ß√£o de Depend√™ncias: As depend√™ncias do projeto, incluindo ferramentas de red teaming como Hydra, Dirb e Nmap, foram instaladas conforme as instru√ß√µes.

Treinamento dos Agentes:

Agente PPO: O agente PPO foi treinado utilizando o ambiente Exoskeleton e NASim, configurando os par√¢metros de n_steps, n_epochs e total_timesteps.

Agente KCU: O agente KCU foi treinado com o ambiente Exoskeleton e NASim, com a DecisionTreePolicy e par√¢metros espec√≠ficos como epsilon, decay_rate, kcc (para ativar o m√≥dulo Kill Chain Catalyst), jeopard, seed, buffer, n_estimators e min_samples_leaf.

Avalia√ß√£o dos Agentes: A performance dos agentes foi avaliada atrav√©s de m√©tricas como o n√∫mero de passos para atingir o objetivo (DatafromLocalSystemSSH e obten√ß√£o das flags user.txt e root.txt), a taxa de sucesso e a efici√™ncia na identifica√ß√£o das kill chains √≥timas. Gr√°ficos de compara√ß√£o (demo-kcc-charts.png, demo-kcc-jeopard-charts.png, combined_charts_kcc_ppo.png, combined_charts_kcu_kcc_tiny.png, combined_charts_kcc_ppo_tiny.png) foram gerados para visualizar o desempenho dos algoritmos em diferentes cen√°rios.

An√°lise Comparativa: Realizou-se uma an√°lise comparativa entre o desempenho do KCU (com e sem KCC) e o PPO em ambos os tipos de ambientes, buscando identificar as vantagens e desvantagens de cada abordagem.

Esta metodologia buscou garantir a robustez dos experimentos e a validade dos resultados, permitindo uma avalia√ß√£o abrangente das capacidades do Exoskeleton na automa√ß√£o de opera√ß√µes de red teaming.

---

## 5. Desenvolvimento

### 5.1 Implementa√ß√£o do Ambiente Exoskeleton
A base do Exoskeleton √© a sua capacidade de atuar como um ambiente de Aprendizado por Refor√ßo (RL). Para isso, foi implementada a classe Exoskeleton (localizada em core.exoskeleton), que se adere √† interface do Gymnasium (anteriormente OpenAI Gym). Essa compatibilidade √© crucial para permitir a integra√ß√£o com bibliotecas de RL populares, como o Stable Baselines3.

A implementa√ß√£o do ambiente Exoskeleton envolveu:

Defini√ß√£o do Espa√ßo de A√ß√µes e Observa√ß√µes: Foi necess√°rio mapear as diversas t√°ticas, t√©cnicas e procedimentos (TTPs) de red teaming em um espa√ßo de a√ß√µes discretas que o agente pudesse escolher. O espa√ßo de observa√ß√µes foi projetado para fornecer ao agente informa√ß√µes relevantes sobre o estado atual do alvo e o progresso do ataque.

Fun√ß√£o de Recompensa: A fun√ß√£o de recompensa foi cuidadosamente elaborada para guiar o agente em dire√ß√£o aos objetivos de red teaming, como a obten√ß√£o de flags (user.txt e root.txt) em cen√°rios de CTF. Recompensas positivas s√£o atribu√≠das ao progresso no kill chain e √† descoberta de informa√ß√µes cr√≠ticas, enquanto penalidades podem ser aplicadas por a√ß√µes ineficazes ou que aumentem o risco de detec√ß√£o.

Fun√ß√£o de Transi√ß√£o de Estado: A l√≥gica para atualizar o estado do ambiente ap√≥s uma a√ß√£o do agente foi implementada, simulando as intera√ß√µes com m√°quinas vulner√°veis e a evolu√ß√£o do cen√°rio de ataque. Isso incluiu a integra√ß√£o com ferramentas de red teaming (como Hydra, Dirb, Nmap) e a simula√ß√£o de seus resultados.

Reset do Ambiente: A funcionalidade de reset() foi implementada para permitir que o ambiente retorne a um estado inicial, possibilitando m√∫ltiplos epis√≥dios de treinamento.

### 5.2 Desenvolvimento do Algoritmo Kill Chain Unscrambler (KCU)
O KCU √© um componente central do Exoskeleton e sua implementa√ß√£o diverge dos algoritmos de RL convencionais ao empregar √°rvores de decis√£o.

DecisionTreePolicy (models.policy): Esta classe foi desenvolvida para implementar a pol√≠tica do KCU, utilizando Random Forest como motor para classifica√ß√£o. A escolha do Random Forest foi motivada pela sua capacidade de lidar com dados complexos e pela interpretabilidade das decis√µes, caracter√≠sticas importantes em cen√°rios de seguran√ßa. O m√©todo fit desta classe √© respons√°vel pelo treinamento do modelo com base nos dados observados.

Classe KCU (models.kcu): Estendendo a BaseAlgorithm do Stable Baselines3, a classe KCU orquestra a funcionalidade geral do algoritmo. Isso inclui a coleta de rollouts, a filtragem das melhores sequ√™ncias (o que √© crucial para o processo de aprendizado otimizado do KCU) e o treinamento da pol√≠tica selecionada (DecisionTreePolicy). Par√¢metros como epsilon, decay_rate, kcc, jeopard, seed, buffer, n_estimators e min_samples_leaf foram implementados para permitir a customiza√ß√£o do comportamento do KCU.

### 5.3 Implementa√ß√£o do Kill Chain Catalyst (KCC)
O KCC √© um m√≥dulo catal√≠tico que otimiza o processo de aprendizado do KCU, e sua implementa√ß√£o √© inspirada em bioinform√°tica.

Alinhamento Gen√©tico: A funcionalidade do KCC envolve a utiliza√ß√£o do algoritmo Needleman-Wunsch para alinhar kill chains, tratando-as como sequ√™ncias de prote√≠nas. Isso permite a identifica√ß√£o de padr√µes e o consenso de sequ√™ncias.

Pondera√ß√£o de T√©cnicas: O KCC gera um vetor de pesos para cada t√©cnica, atribuindo import√¢ncia a cada item na sequ√™ncia dentro de uma amostra de treinamento. Essa pondera√ß√£o atua como um processo enzim√°tico, acelerando a busca por sequ√™ncias √≥timas de kill chain. A integra√ß√£o do KCC ao KCU foi realizada de forma a influenciar o processo de sele√ß√£o e filtragem das melhores sequ√™ncias durante o treinamento.

### 5.4 Configura√ß√£o dos Ambientes de Teste
Para garantir um ambiente de testes isolado e replic√°vel, os cen√°rios dummy e CVE-2018-10993 foram empacotados em cont√™ineres Docker. A configura√ß√£o via docker-compose facilitou a inicializa√ß√£o e o gerenciamento dos ambientes, garantindo que as depend√™ncias e as vulnerabilidades estivessem corretamente configuradas para cada cen√°rio.

### 5.5 Agentes de Teste
Al√©m dos agentes KCU e PPO que interagem com o ambiente Exoskeleton, foram implementados agentes de linha de base para compara√ß√£o:

Jeopard Agent (agent_jeopard.py): Um agente que segue uma sequ√™ncia de a√ß√µes est√°tica e pr√©-definida. Este agente utiliza a API nativa do Exoskeleton.

Random Agent (agent_random.py): Um agente que executa a√ß√µes aleat√≥rias iterativamente at√© atingir o objetivo. Este agente tamb√©m utiliza a API nativa do Exoskeleton.

O desenvolvimento incluiu a configura√ß√£o de scripts para a execu√ß√£o desses agentes, bem como para os agentes PPO e KCU, tanto com o ambiente Exoskeleton quanto com o ambiente NASim, demonstrando a flexibilidade do framework.

Em resumo, o processo de desenvolvimento focou na cria√ß√£o de um ambiente de RL robusto e flex√≠vel, na implementa√ß√£o inovadora do KCU com √°rvores de decis√£o e na integra√ß√£o do KCC para otimizar o aprendizado, tudo isso validado em ambientes de teste controlados.

---

## 6. Resultados e Discuss√µes

Este cap√≠tulo apresenta os resultados obtidos nos experimentos conduzidos com o projeto Exoskeleton, comparando o desempenho dos algoritmos KCU (com e sem KCC), PPO, Jeopard e Random Agents. As discuss√µes se concentram na efic√°cia dos algoritmos em identificar e executar kill chains em cen√°rios de red teaming, bem como nas vantagens da abordagem proposta.

### 6.1 Desempenho no Cen√°rio dummy
O cen√°rio dummy foi utilizado para uma avalia√ß√£o inicial do desempenho dos agentes em um ambiente controlado e relativamente simples, com uma kill chain √≥tima predefinida.

A an√°lise dos resultados demonstra que o KCU, especialmente quando auxiliado pelo m√≥dulo KCC, exibiu uma converg√™ncia acelerada da curva de aprendizado. O KCC, ao aplicar o conceito de alinhamento gen√©tico para ponderar as t√©cnicas da kill chain, permitiu que o KCU identificasse as sequ√™ncias √≥timas de forma mais eficiente, com um menor n√∫mero de passos e uma taxa de sucesso mais consistente em compara√ß√£o com o KCU puro. Isso valida a premissa de que o KCC atua como um catalisador, otimizando a busca por sequ√™ncias ideais.

A integra√ß√£o do Jeopard Mode com o KCC no cen√°rio dummy revelou um comportamento interessante. O Jeopard Agent, por seguir uma sequ√™ncia de a√ß√µes est√°tica e pr√©-definida, serve como uma linha de base para comparar a capacidade de adapta√ß√£o dos agentes baseados em RL. Quando o KCU com KCC √© combinado com a estrat√©gia do Jeopard, observou-se uma melhora na estabilidade e na rapidez na obten√ß√£o do objetivo em alguns casos. Isso sugere que a incorpora√ß√£o de um conhecimento pr√©vio (estrat√©gia Jeopard) pode, em certas condi√ß√µes, guiar o agente de RL de forma mais eficaz no in√≠cio do aprendizado, antes que o algoritmo de RL descubra a pol√≠tica √≥tima de forma aut√¥noma.

A compara√ß√£o entre o KCU (com KCC) e o PPO no cen√°rio dummy destacou a superioridade do KCU em termos de desempenho na sequencia√ß√£o de tarefas. Enquanto o PPO, um algoritmo de RL baseado em redes neurais, demonstra capacidade de aprendizado, o KCU, com sua abordagem baseada em √°rvores de decis√£o e otimizado pelo KCC, apresentou uma converg√™ncia mais r√°pida para a solu√ß√£o √≥tima e uma maior robustez na identifica√ß√£o das kill chains. Essa diferen√ßa √© particularmente relevante em ambientes onde a interpretabilidade e a efici√™ncia do aprendizado com dados esparsos s√£o cruciais.

### 6.2 Desempenho no Cen√°rio NASim (tiny)
O ambiente NASim (tiny) representa um cen√°rio de rede mais abstrato, permitindo avaliar a generaliza√ß√£o e a efic√°cia dos algoritmos em um contexto diferente do Exoskeleton direto.

Similarmente ao cen√°rio dummy, a adi√ß√£o do KCC ao KCU no ambiente NASim resultou em uma melhoria not√°vel no desempenho. O KCC contribuiu para uma otimiza√ß√£o mais r√°pida da pol√≠tica, com o KCU alcan√ßando o objetivo com menos itera√ß√µes ou em menos tempo de treinamento. Isso refor√ßa a efic√°cia do KCC como um acelerador do aprendizado, independentemente da complexidade intr√≠nseca do ambiente de rede.

A compara√ß√£o entre KCC e PPO no ambiente NASim reiterou a vantagem do KCU (com KCC) sobre o PPO. Embora o PPO demonstre a capacidade de aprender a navegar e atacar no NASim, o KCU com KCC consistentemente convergiu mais rapidamente para pol√≠ticas eficazes. Isso sugere que a abordagem do KCU, focada em √°rvores de decis√£o e otimizada pelo KCC, √© mais robusta para identificar sequ√™ncias de ataque em ambientes estoc√°sticos e din√¢micos, onde a efici√™ncia e a precis√£o do sequenciamento s√£o primordiais para evitar a detec√ß√£o e alcan√ßar os objetivos.

### 6.3 Discuss√£o Geral
Os resultados demonstram de forma consistente a efic√°cia do projeto Exoskeleton na automa√ß√£o de opera√ß√µes de red teaming e a superioridade do algoritmo KCU, especialmente quando aprimorado pelo Kill Chain Catalyst (KCC). A abordagem do KCU, que utiliza √°rvores de decis√£o em vez de redes neurais, revelou-se particularmente vantajosa para a tarefa de sequenciamento de ataques em ambientes estoc√°sticos e desconhecidos, caracter√≠stica dos cen√°rios de CTF e red teaming. A capacidade do KCU de aprender e aplicar padr√µes de ataque de forma eficiente √© um diferencial significativo.

O Kill Chain Catalyst (KCC) provou ser um componente crucial na acelera√ß√£o do processo de aprendizado do KCU. Sua inspira√ß√£o em bioinform√°tica e a aplica√ß√£o de alinhamento gen√©tico para ponderar as t√©cnicas da kill chain resultaram em uma converg√™ncia mais r√°pida e em sequ√™ncias de ataque mais otimizadas, minimizando erros e o n√∫mero de passos. Isso √© de extrema import√¢ncia para opera√ß√µes de red teaming, onde a furtividade e a efici√™ncia s√£o vitais.

Em compara√ß√£o com algoritmos de RL convencionais como o PPO, o KCU (com KCC) demonstrou maior adaptabilidade e efici√™ncia na identifica√ß√£o de sequ√™ncias de ataque em cen√°rios de red teaming. Enquanto o PPO pode exigir um volume maior de dados e tempo de treinamento, a abordagem do KCU parece ser mais robusta para lidar com as especificidades dos ambientes de ciberseguran√ßa.

A capacidade do Exoskeleton de se integrar com ambientes de mundo real, como Vulnhub, Hack The Box (HTB) e TryHackMe (THM) no futuro, al√©m de cont√™ineres Docker pr√©-configurados, sublinha seu potencial para aplica√ß√µes pr√°ticas e sua relev√¢ncia para a pesquisa em seguran√ßa cibern√©tica.

Em suma, os resultados obtidos refor√ßam o potencial do Exoskeleton como uma ferramenta poderosa para a automa√ß√£o de red teaming, com o KCU e o KCC representando avan√ßos significativos na aplica√ß√£o de Aprendizado por Refor√ßo para a otimiza√ß√£o de estrat√©gias de ataque.

---

## 7. Apresenta√ß√£o dos Gr√°ficos e Execu√ß√£o

Este cap√≠tulo destina-se √† apresenta√ß√£o visual dos resultados e √† demonstra√ß√£o da execu√ß√£o dos algoritmos implementados no projeto Exoskeleton. As imagens a seguir s√£o prints de tela que retratam o desempenho dos diferentes agentes e a visualiza√ß√£o das kill chains geradas.

### 7.1 Prints de Tela da Execu√ß√£o
As imagens a seguir capturam momentos chave da execu√ß√£o dos algoritmos, demonstrando a intera√ß√£o dos agentes com os ambientes de teste e o progresso em dire√ß√£o aos objetivos de red teaming.

Imagem 7.2.1: Execu√ß√£o do Agente KCU no Cen√°rio Dummy
Inserir print de tela da execu√ß√£o do python agent_kcu.py no cen√°rio dummy, mostrando a sa√≠da no terminal ou logs relevantes.

Imagem 7.2.2: Execu√ß√£o do Agente PPO no Cen√°rio Dummy
Inserir print de tela da execu√ß√£o do python agent_ppo.py no cen√°rio dummy, mostrando a sa√≠da no terminal ou logs relevantes.

Imagem 7.2.3: In√≠cio do Cen√°rio CVE-2018-10993 via Docker Compose
Inserir print de tela da inicializa√ß√£o do docker-compose up no diret√≥rio scenarios/CVE-2018-10993.

Imagem 7.2.4: Agente Jeopard em A√ß√£o
Inserir print de tela da execu√ß√£o do python agent_jeopard.py, mostrando a sequ√™ncia est√°tica de a√ß√µes.

Imagem 7.2.5: Agente Random em A√ß√£o
Inserir print de tela da execu√ß√£o do python agent_random.py, mostrando a natureza aleat√≥ria das a√ß√µes.

---

## 8. Considera√ß√µes Finais

Este trabalho apresentou o projeto Exoskeleton, uma plataforma inovadora para a automa√ß√£o de opera√ß√µes de red teaming em cen√°rios de Capture The Flag (CTF), atuando como uma camada de abstra√ß√£o entre algoritmos de Aprendizado por Refor√ßo (RL) e m√°quinas vulner√°veis. A pesquisa explorou a efic√°cia do algoritmo Kill Chain Unscrambler (KCU) e do m√≥dulo Kill Chain Catalyst (KCC), comparando seu desempenho com algoritmos de RL convencionais, como o PPO, em diversos ambientes de teste.

Os resultados obtidos demonstraram consistentemente a superioridade do KCU, especialmente quando otimizado pelo KCC, na identifica√ß√£o e execu√ß√£o de sequ√™ncias de ataque eficientes e furtivas. A abordagem do KCU, que emprega √°rvores de decis√£o em vez de redes neurais, revelou-se particularmente adaptada para a natureza estoc√°stica e din√¢mica dos ambientes de red teaming, permitindo uma converg√™ncia de aprendizado mais r√°pida e robusta em compara√ß√£o com o PPO. O KCC, inspirado em bioinform√°tica, comprovou sua capacidade de acelerar a otimiza√ß√£o das kill chains, minimizando o n√∫mero de passos e os erros, o que √© crucial para opera√ß√µes de red teaming realistas.

A capacidade do Exoskeleton de integrar diversas t√°ticas, t√©cnicas e procedimentos (TTPs) de red teaming e de operar em ambientes controlados via Docker, com potencial para integra√ß√£o futura com plataformas como Vulnhub, Hack The Box (HTB) e TryHackMe (THM), ressalta sua relev√¢ncia pr√°tica e seu potencial para pesquisa avan√ßada em seguran√ßa cibern√©tica. Este projeto contribui significativamente para a automa√ß√£o do red teaming, oferecendo uma ferramenta promissora para a detec√ß√£o proativa de vulnerabilidades e o fortalecimento das defesas cibern√©ticas.

### 8.1 Limita√ß√µes e Trabalhos Futuros
Apesar dos resultados promissores, o projeto Exoskeleton, ainda em desenvolvimento ativo, apresenta algumas limita√ß√µes e abre portas para futuros trabalhos:

Complexidade de Cen√°rios: Atualmente, os cen√°rios de teste s√£o pr√©-configurados. A escalabilidade do Exoskeleton para ambientes de rede em larga escala e com maior complexidade, incluindo intera√ß√µes mais din√¢micas e imprevis√≠veis, √© um desafio a ser explorado.

Adapta√ß√£o a Novas Vulnerabilidades: A capacidade do Exoskeleton de se adaptar automaticamente a novas vulnerabilidades e TTPs n√£o explicitamente programadas ainda precisa ser aprofundada. A integra√ß√£o de t√©cnicas de aprendizado por transfer√™ncia ou meta-learning pode ser explorada.

Interpretabilidade Aprofundada: Embora o KCU utilize √°rvores de decis√£o para maior interpretabilidade em compara√ß√£o com redes neurais, uma an√°lise mais aprofundada das decis√µes tomadas pelo agente e a justificativa para a sele√ß√£o de certas kill chains podem ser investigadas para aumentar a confian√ßa e a auditoria das opera√ß√µes.

Otimiza√ß√£o do KCC para Diferentes Escalas: A influ√™ncia do KCC no KCU foi demonstrada em cen√°rios espec√≠ficos. Pesquisas futuras podem investigar a otimiza√ß√£o dos par√¢metros do KCC e sua adaptabilidade a diferentes escalas e complexidades de kill chains.

### 8.2 Proposta de Varia√ß√£o do Algoritmo KCC para Pesquisa de Mestrado
Como objeto de pesquisa para o mestrado, proponho uma varia√ß√£o do algoritmo Kill Chain Catalyst (KCC), denominada KCC-Adaptativo (KCC-A). O objetivo principal do KCC-A √© aprimorar a capacidade de adapta√ß√£o do KCC em cen√°rios de red teaming altamente din√¢micos e desconhecidos, onde as caracter√≠sticas das kill chains ideais podem mudar rapidamente.

A inova√ß√£o do KCC-A reside em duas frentes principais:

Mecanismo de Pondera√ß√£o Din√¢mica de T√©cnicas: Em vez de um vetor de pesos est√°tico gerado pelo consenso de sequ√™ncias (como no KCC original), o KCC-A introduziria um mecanismo de pondera√ß√£o adaptativo. Este mecanismo ajustaria os pesos das t√©cnicas de ataque em tempo real, com base no feedback cont√≠nuo do ambiente (e.g., sucesso ou falha de uma a√ß√£o, tempo decorrido, detec√ß√£o) e na incerteza associada a cada t√©cnica. Isso poderia ser implementado atrav√©s de algoritmos de aprendizado online ou multi-armed bandits, permitindo que o KCC-A priorize t√©cnicas que demonstrem maior efic√°cia ou menor risco de detec√ß√£o em um determinado contexto.

Alinhamento Gen√©tico Multidimensional e Aprendizado de Recursos (Feature Learning): O KCC atual utiliza o algoritmo Needleman-Wunsch para alinhamento gen√©tico das kill chains como prote√≠nas. O KCC-A expandiria essa abordagem para um alinhamento multidimensional, considerando n√£o apenas a sequ√™ncia das t√©cnicas, mas tamb√©m atributos adicionais, como o custo computacional, o n√≠vel de sigilo esperado, e a probabilidade de sucesso de cada t√©cnica. Al√©m disso, o KCC-A exploraria t√©cnicas de aprendizado de recursos (feature learning) para extrair automaticamente caracter√≠sticas relevantes das kill chains e do ambiente, enriquecendo o processo de alinhamento e pondera√ß√£o. Isso poderia envolver o uso de embeddings para representar as t√©cnicas, permitindo uma similaridade mais rica e contextualizada.

A expectativa √© que o KCC-A, ao incorporar essas capacidades adaptativas, melhore significativamente a robustez do Exoskeleton em cen√°rios de red teaming do mundo real, onde a din√¢mica das amea√ßas exige uma capacidade de resposta e adapta√ß√£o cont√≠nuas. Esta varia√ß√£o do algoritmo ser√° minuciosamente investigada, implementada e avaliada como parte da minha disserta√ß√£o de mestrado.

---

## 9. Refer√™ncias
[1] HORTA, Antonio Jose Neto; DOS SANTOS, Anderson Fernandes Pereira; GOLDSHMIDT, Ronaldo. Kill Chain Catalyst for Autonomous Red Team Operations in Dynamic Attack Scenarios. In: Simp√≥sio Brasileiro de Seguran√ßa da Informa√ß√£o e de Sistemas Computacionais (SBSeg), 2024. Dispon√≠vel em: https://doi.org/10.5753/sbseg.2024.241371.

```bibtex
@thesis{reis2025kcc,
  author  = {Reis, Tiago Marcelino},
  title   = {KCC-A Uma Varia√ß√£o do Algoritmo KCC para Opera√ß√µes de Red Teaming},
  school  = {Universidade Federal de Goi√°s},
  year    = {2025},
  type    = {Trabalho para a disciplina T√≥picos Especiais em Redes}
}

